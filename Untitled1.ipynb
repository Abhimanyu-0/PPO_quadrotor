{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd8b690-8f30-42e3-b2fa-1df1fc81b4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mass    = 0.5\n",
      "Length  = 0.15\n",
      "Inertia = 0.1\n",
      "Dt      = 0.04\n",
      "state size   = 6\n",
      "control size = 2\n"
     ]
    }
   ],
   "source": [
    "import quadrotor\n",
    "\n",
    "print(\"Mass    =\", quadrotor.MASS)\n",
    "print(\"Length  =\", quadrotor.LENGTH)\n",
    "print(\"Inertia =\", quadrotor.INERTIA)\n",
    "print(\"Dt      =\", quadrotor.DT)\n",
    "print(\"state size   =\", quadrotor.DIM_STATE)\n",
    "print(\"control size =\", quadrotor.DIM_CONTROL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b2d4c2a-0684-41bd-9121-1bf6f4197133",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (529514410.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    The goal of this project is to learn a policy that can move the robot from any point to the red dot ($x^{\\star} = [2, 0, 0, 0, 0, 0]^T$) while avoiding thee obstacles. The obstacles are represented by the black circles in the animation. You can check if the drone is in collision with an obstacle using the function ```quadrotor.check_collision```.\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The goal of this project is to learn a policy that can move the robot from any point to the red dot ($x^{\\star} = [2, 0, 0, 0, 0, 0]^T$) while avoiding thee obstacles. The obstacles are represented by the black circles in the animation. You can check if the drone is in collision with an obstacle using the function ```quadrotor.check_collision```. \n",
    "\n",
    "## Create a RL environment\n",
    "Using the [stable_baselines3](https://stable-baselines3.readthedocs.io/en/master/), create a [custom RL environment](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html) environment. You will have to follow the following steps:\n",
    "\n",
    "1. Implement a step function than contrains the dynamics (you are free to use the ```quadrotor.next_state```) and a reward function. To speed-up the training, make sure to add a gravity compensation term in your dynamics (i.e. the drone should stay in place when the policy outputs zeros).\n",
    "   The reward should be made of three terms:\n",
    "   \n",
    "- A positive term to incentivize the quadrotor to reach the target. You can start with a reward bounded between 0 and 1, e.g.\n",
    "   $\\operatorname{exp}(-\\frac{1}{2} (x - x^{\\star})Q(x - x^{\\star}) -\\frac{1}{2} (u - u_{\\text{gravity}})R(u - u_{\\text{gravity}}))$\n",
    "\n",
    "   \n",
    "- A large negative penality(e.g. -100)  if the robot get out of the following bounds:\n",
    "$ p_x \\in [-4, 4], \\quad v_x \\in [-10, 10] , \\quad p_y \\in [-4, 4] , \\quad v_y \\in [-10, 10] , \\quad \\theta \\in [-2 \\pi, 2 \\pi] , \\quad \\omega \\in [-10, 10] $.\n",
    "\n",
    " - A negative penalty if the robot hits the obstacle, e.g. -1. You should use the ```quadrotor.check_collision```.\n",
    "\n",
    "Keep in mind that, in RL, the goal is to maximize a reward (and not minimize a cost like in Optimal Control).\n",
    "\n",
    "2. Implement a reset function that initializes the state randomly. You can sample uniformly between $[-2, 2]$ for $p_x$ and $p_y$ and initialize the other terms to zero. Make sure to reject samples that are colliding with the obstacles using the ```quadrotor.check_collision```.\n",
    "\n",
    "3. In the step function, stop the environment using ```truncated``` after 200 steps (Here is an [example](https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb)).\n",
    "\n",
    "4. In the step function, stop the environment if the drone goes outsite of the provided bounds using ```terminated``` (Here is an [example](https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb)).\n",
    "\n",
    "5. Make sure that your environment is well defined using the ```check_env``` function.\n",
    "   \n",
    "## Training a policy with PPO   \n",
    "Train a policy with PPO and use the learned policy to define a controller. Make sure that you can reach the target while avoiding the obstacles starting from $x_0 = [-2, 0, 0, 0 ,0, 0]$\n",
    "\n",
    "\n",
    "\n",
    "Please submit your code (as runnable Jupyter Notebook), a pdf report and an mp4 video. In the report, explain your reward design and provide plots showing the trajectory of the quadrotor. The mp4 video should show the quadrotor animation starting from $x_0 = [-2, 0, 0, 0 ,0, 0]$. You can save your animation in the following way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18eb449-307d-418d-a934-9fd0eecce26e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
